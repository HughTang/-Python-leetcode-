# Linux的I/O模式以及select,poll,epoll（Socket）

## 概念说明

\- 用户空间和内核空间
\- 进程切换
\- 进程的阻塞
\- 文件描述符
\- 缓存 I/O

### 进程切换

从一个进程的运行转到另一个进程上运行，这个过程中经过下面这些变化：

1. 保存处理机上下文，包括程序计数器和其他寄存器。

2. 更新PCB信息。

3. 把进程的PCB移入相应的队列，如就绪、在某事件阻塞等队列。

4. 选择另一个进程执行，并更新其PCB。

5. 更新内存管理的数据结构。

6. 恢复处理机上下文。

### 进程的阻塞

正在执行的进程，由于期待的某些事件未发生，如请求系统资源失败、等待某种操作的完成、新数据尚未到达或无新工作做等，则由系统自动执行阻塞原语(Block)，使自己由运行状态变为阻塞状态

进程的阻塞是进程自身的一种主动行为，也因此只有处于运行态的进程（获得CPU），才可能将其转为阻塞状态。`当进程进入阻塞状态，是不占用CPU资源的`。

### 文件描述符fd

用于表述指向文件的引用的抽象化概念。

文件描述符在形式上是一个非负整数。实际上，它是一个索引值，指向内核为每一个进程所维护的该进程打开文件的记录表。当程序打开一个现有文件或者创建一个新文件时，内核向进程返回一个文件描述符。

文件描述符这一概念往往只适用于UNIX、Linux这样的操作系统。

### 缓存 I/O

缓存 I/O 又被称作标准 I/O，大多数文件系统的默认 I/O 操作都是缓存 I/O。在 Linux 的缓存 I/O 机制中，操作系统会将 I/O 的数据缓存在文件系统的页缓存（ page cache ）中，也就是说，数据会先被拷贝到操作系统内核的缓冲区中，然后才会从操作系统内核的缓冲区拷贝到应用程序的地址空间。

**缓存 I/O 的缺点：**
数据在传输过程中需要在应用程序地址空间和内核进行多次数据拷贝操作，这些数据拷贝操作所带来的 CPU 以及内存开销是非常大的。



## IO模式

当一个read操作发生时，它会经历两个阶段：

1. 等待数据准备 (Waiting for the data to be ready)

2. 将数据从内核拷贝到进程中 (Copying the data from the kernel to the process)

正式因为这两个阶段，linux系统产生了下面五种网络模式的方案。
\- 阻塞 I/O（blocking IO）
\- 非阻塞 I/O（nonblocking IO）
\- I/O 多路复用（ IO multiplexing）
\- 信号驱动 I/O（ signal driven IO）
\- 异步 I/O（asynchronous IO）

### 阻塞 I/O（blocking IO）

在linux中，默认情况下所有的socket都是blocking

I/O两个阶段，如果数据没有到达（没有足够的数据），用户进程和kernel会分别阻塞

blocking IO的特点就是在IO执行的两个阶段都被block了。



### 非阻塞 I/O（nonblocking IO）

linux下，可以通过设置socket使其变为non-blocking。

当用户进程发出read操作时，如果kernel中的数据还没有准备好，那么它并不会block用户进程，而是立刻返回一个error。

nonblocking IO的特点是用户进程需要**不断的主动询问**kernel数据好了没有。



### I/O 多路复用（ IO multiplexing）

![img](https://pic4.zhimg.com/v2-0a86ab90d8167860dec5c695064648f3_b.jpg)

IO multiplexing就是我们说的select，poll，epoll，有些地方也称这种IO方式为event driven IO。

select/epoll的好处就在于单个process就可以同时处理多个网络连接的IO。它的基本原理就是select，poll，epoll这个function会不断的轮询所负责的所有socket，当某个socket有数据到达了，就通知用户进程。

`当用户进程调用了select，那么整个进程会被block`，而同时，kernel会“监视”所有select负责的socket，当任何一个socket中的数据准备好了，select就会返回。这个时候用户进程再调用read操作，将数据从kernel拷贝到用户进程。

所以，I/O 多路复用的特点是通过一种机制一个进程能同时等待多个文件描述符，而这些文件描述符（套接字描述符）其中的任意一个进入读就绪状态，select()函数就可以返回。

如果处理的连接数不是很高的话，使用select/epoll的web server不一定比使用multi-threading + blocking IO的web server性能更好，可能延迟还更大。select/epoll的优势并不是对于单个连接能处理得更快，而是在于能处理更多的连接。）

在IO multiplexing Model中，实际中，对于每一个socket，一般都设置成为non-blocking

![](https://i.loli.net/2021/03/30/H8TAXGbeNzSqpyU.png)

### 信号驱动 I/O

应用进程使用 sigaction 系统调用，内核立即返回，应用进程可以继续执行，也就是说等待数据阶段应用进程是非阻塞的。内核在数据到达时向应用进程发送 SIGIO 信号，应用进程收到之后在信号处理程序中调用 recvfrom 将数据从内核复制到应用进程中。

相比于非阻塞式 I/O 的轮询方式，信号驱动 I/O 的 CPU 利用率更高。



### 异步 I/O（asynchronous IO）

用户进程发起read操作之后，立刻就可以开始去做其它的事。而另一方面，从kernel的角度，当它受到一个asynchronous read之后，首先它会立刻返回，所以不会对用户进程产生任何block。然后，kernel会等待数据准备完成，然后将数据拷贝到用户内存，当这一切都完成之后，kernel会给用户进程发送一个signal，告诉它read操作完成了。



### 五大 I/O 模型比较

- 同步 I/O：将数据从内核缓冲区复制到应用进程缓冲区的阶段（第二阶段），应用进程会阻塞。
- 异步 I/O：第二阶段应用进程不会阻塞。

同步 I/O 包括阻塞式 I/O、非阻塞式 I/O、I/O 复用和信号驱动 I/O ，它们的主要区别在第一个阶段。

非阻塞式 I/O 、信号驱动 I/O 和异步 I/O 在第一阶段不会阻塞。

![](https://i.loli.net/2021/03/30/tkQuN3jACqpFgBS.png)



## select、poll、epoll详解

select，poll，epoll本质上都是同步I/O，因为他们都需要在读写事件就绪后自己负责进行读写

### select

select的底层是一个fd_set的数据结构，本质上是一个long类型的数组，数组中每一个元素都对应于一个文件描述符，通过轮询所有的文件描述符来检查是否有事件发生。

【优点】：

可移植性好；

连接数少并且连接都十分活跃的情况下，效率也不错。

【缺点】：

可以监听的最大文件描述符数量为1024（因为内核写定了）。

检查是否有事件发生是采用轮询遍历的方式，当文件描述符很多时开销很大。

### poll

poll与select差不多，但**poll的文件描述符没有最大数量的限制**，但是**依然采用轮询遍历**的方式检查是否有事件发生。

### 比较

#### 1. 功能

select 和 poll 的功能基本相同，不过在一些实现细节上有所不同。

- select 会修改描述符，而 poll 不会；
- select 的描述符类型使用数组实现，FD_SETSIZE 大小默认为 1024，因此默认只能监听少于 1024 个描述符。如果要监听更多描述符的话，需要修改 FD_SETSIZE 之后重新编译；而 poll 没有描述符数量的限制；
- poll 提供了更多的事件类型，并且对描述符的重复利用上比 select 高。
- 如果一个线程对某个描述符调用了 select 或者 poll，另一个线程关闭了该描述符，会导致调用结果不确定。

#### 2. 速度

select 和 poll 速度都比较慢，每次调用都需要将全部描述符从应用进程缓冲区复制到内核缓冲区。

#### 3. 可移植性

几乎所有的系统都支持 select，但是只有比较新的系统支持 poll。





### epoll

![img](https://pic3.zhimg.com/v2-c7246e74cc64c786360ae12e266e0362_b.jpg)

 epoll是一种更加高效的IO多路复用的方式，它**可以监视的文件描述符数量突破了1024的限制（十万）**，同时**不需要通过轮询遍历的方式去检查文件描述符上是否有事件发生，因为epoll_wait返回的就是有事件发生的文件描述符**。本质上是事件驱动。

  具体是通过**红黑树和就绪链表**实现的，红黑树存储所有的文件描述符，就绪链表存储有事件发生的文件描述符；

- **epoll_ctl**可以对文件描述符结点进行增、删、改、查，并且**告知内核注册回调函数（事件）**。

- 一旦**文件描述符上有事件发生时，那么内核将该文件描述符节点插入到就绪链表里面**

- 这时候epoll_wait将会接收到消息，并且**将数据拷贝到用户空间**。

  表面上看epoll的性能最好，但是在连接数少并且连接都十分活跃的情况下，select和poll的性能可能比epoll好，毕竟epoll的通知机制需要很多函数回调。

epoll 仅适用于 **Linux OS**。

#### epoll读到一半又有新事件来了怎么办？

避免在主进程epoll再次监听到同一个可读事件，可以把对应的描述符设置为**EPOLL_ONESHOT**，效果是监听到一次事件后就将对应的描述符从监听集合中移除，也就不会再被追踪到。读完之后可以再把对应的描述符重新手动加上。



### 工作模式

epoll 的描述符事件有两种触发模式：LT（level trigger）和 ET（edge trigger）。

#### 1. LT 模式

当 epoll_wait() 检测到描述符事件到达时，将此事件通知进程，进程可以不立即处理该事件，下次调用 epoll_wait() 会再次通知进程。是默认的一种模式，并且同时支持 Blocking 和 No-Blocking。

#### 2. ET 模式

和 LT 模式不同的是，通知之后进程必须立即处理事件，下次再调用 epoll_wait() 时不会再得到事件到达的通知。

很大程度上减少了 epoll 事件被重复触发的次数，因此效率要比 LT 模式高。只支持 No-Blocking，以避免由于一个文件句柄的阻塞读/阻塞写操作把处理多个文件描述符的任务饿死。

### 应用场景

#### 1.select 应用场景

select 的 timeout 参数精度为微秒，而 poll 和 epoll 为毫秒，因此 select 更加适用于实时性要求比较高的场景，比如核反应堆的控制。

select 可移植性更好，几乎被所有主流平台所支持。

#### 2. poll 应用场景

poll 没有最大描述符数量的限制，如果平台支持并且对实时性要求不高，应该使用 poll 而不是 select。

#### 3. epoll 应用场景

只需要运行在 Linux 平台上，有大量的描述符需要同时轮询，并且这些连接最好是长连接。

需要同时监控小于 1000 个描述符，就没有必要使用 epoll，因为这个应用场景下并不能体现 epoll 的优势。

需要监控的描述符状态变化多，而且都是非常短暂的，也没有必要使用 epoll。因为 epoll 中的所有描述符都存储在内核中，造成每次需要对描述符的状态改变都需要通过 epoll_ctl() 进行系统调用，频繁系统调用降低效率。并且 epoll 的描述符存储在内核，不容易调试。





# 零拷贝

## **一、什么是零拷贝？**

### **1、从一个案例说起**

我们先要从一个需求说起，说某天某领导给你下发了一个任务，完成一个从文件中读取数据，并传输到网络上的一个小程序。代码很简单：

> 首先我们在我们的操作系统中找到这个文件，然后把数据先读到缓冲区，最后把缓冲区的数据发送到网络上。

代码是很简单，现在我们考虑一下，这个数据从电脑到网络整个传输的过程：

![img](http://pics1.baidu.com/feed/ca1349540923dd54e4b98972819036d89d824805.jpeg?token=654828cc7481f05ac9f0d3411d2a7430)

现在我们可以看到1->2->3->4的整个过程一共经历了四次拷贝的方式，**但是真正消耗资源和浪费时间的是第二次和第三次，因为这两次都需要经过我们的CPU拷贝，而且还需要内核态和用户态之间的来回切换。**想想看，我们的CPU资源是多么宝贵，要处理大量的任务。还要去拷贝大量的数据。如果能把CPU的这两次拷贝给去除掉，岂不快哉！！！既能节省CPU资源，还可以避免内核态和用户态之间的切换。

这里还要先说一下用户态和内核态的区别：

> 处于用户态执行时，进程所能访问的内存空间和对象受到限制，其所处于占有的处理器是可被抢占的。
>
> 处于内核态执行时，则能访问所有的内存空间和对象，且所占有的处理器是不允许被抢占的。



### **2、优化方案**

#### 1) 利用mmap()

 在 Linux 中，减少拷贝次数的一种方法是调用 mmap() 来代替调用 **read(第二次拷贝)**，比如：

```java
tmp_buf = mmap(file, len);  

write(socket, tmp_buf, len);
```

应用程序调用`mmap()`，磁盘上的数据会通过`DMA`被拷贝的内核缓冲区，接着操作系统会把这段内核缓冲区与应用程序共享，这样就不需要把内核缓冲区的内容往用户空间拷贝。应用程序再调用`write()`,操作系统直接将内核缓冲区的内容拷贝到`socket`缓冲区中，这一切都发生在内核态，最后，`socket`缓冲区再把数据发到网卡去。

![图 2. 利用 mmap() 代替 read()](http://www.ibm.com/developerworks/cn/linux/l-cn-zerocopy2/image002.jpg)

使用 mmap() 其实是存在潜在的问题的。当对文件进行了内存映射，然后调用 write() 系统调用，如果此时其他的进程截断了这个文件，那么 write() 系统调用将会被总线错误信号 SIGBUS 中断，因为此时正在执行的是一个错误的存储访问。这个信号将会导致进程被杀死，解决这个问题可以通过以下这两种方法：

1. 为 SIGBUS 安装一个新的信号处理器，这样，write() 系统调用在它被中断之前就返回已经写入的字节数目，errno 会被设置成 success。但是这种方法也有其缺点，它不能反映出产生这个问题的根源所在，因为 BIGBUS 信号只是显示某进程发生了一些很严重的错误。
2. 第二种方法是通过文件租借锁来解决这个问题的，这种方法相对来说更好一些。我们可以通过内核对文件加读或者写的租借锁，当另外一个进程尝试对用户正在进行 传输的文件进行截断的时候，内核会发送给用户一个实时信号：RT_SIGNAL_LEASE 信号，这个信号会告诉用户内核破坏了用户加在那个文件上的写或者读租借锁，那么 write() 系统调用则会被中断，并且进程会被 SIGBUS 信号杀死，返回值则是中断前写的字节数，errno 也会被设置为 success。文件租借锁需要在对文件进行内存映射之前设置。



#### 2) 利用sendfile()

在linux 2.1内核中，添加了 “**数据被copy到socket buffe**r”（sendfile()）的动作，于是我们的javaNIO，可以直接调用transferTo()的方法，就可以实现这种现象。

![](https://i.loli.net/2021/04/10/brMuXmiywlcfWtv.jpg)

#### 3) 利用splice()

在Linux2.4 内核做了优化，取而代之的是只包含关于数据的位置和长度的信息的描述符被追加到了socket buffer 缓冲区中。**DMA引擎直接把数据从内核缓冲区传输到协议引擎**（protocol engine），从而消除了最后一次CPU copy。经过上述过程，数据只经过了2次copy就从磁盘传送出去了。这个才是真正的Zero-Copy

> **注意：这里的零拷贝其实是根据内核状态划分的，在这里没有经过CPU的拷贝，数据在用户态的状态下，经历了零次拷贝，所以才叫做零拷贝，但不是说不拷贝。**

![](https://i.loli.net/2021/04/10/e98fIynujwtPzS4.jpg)

sendfile只适用于将数据从文件拷贝到套接字上，限定了它的使用范围。Linux在`2.6.17`版本引入`splice`系统调用，用于在两个文件描述符中移动数据：

```
#define _GNU_SOURCE         /* See feature_test_macros(7) */#include <fcntl.h>ssize_t splice(int fd_in, loff_t *off_in, int fd_out, loff_t *off_out, size_t len, unsigned int flags);
```

splice调用在两个文件描述符之间移动数据，而不需要数据在内核空间和用户空间来回拷贝。他从`fd_in`拷贝`len`长度的数据到`fd_out`，但是有一方必须是管道设备，这也是目前`splice`的一些局限性。

splice调用利用了Linux提出的管道缓冲区机制， 所以至少一个描述符要为管道。

## **二、哪些地方会用到零拷贝技术**

### **1、java的NIO**

先说java，是因为要给下面的netty做铺垫，在 Java NIO 中的通道（Channel）就相当于操作系统的内核空间（kernel space）的缓冲区，而缓冲区（Buffer）对应的相当于操作系统的用户空间（user space）中的用户缓冲区（user buffer）。

堆外内存（DirectBuffer）在使用后需要应用程序手动回收，而堆内存（HeapBuffer）的数据在 GC 时可能会被自动回收。因此，在使用 HeapBuffer 读写数据时，为了避免缓冲区数据因为 GC 而丢失，NIO 会先把 HeapBuffer 内部的数据拷贝到一个临时的 DirectBuffer 中的本地内存（native memory），这个拷贝涉及到 sun.misc.Unsafe.copyMemory() 的调用，背后的实现原理与 memcpy() 类似。 最后，将临时生成的 DirectBuffer 内部的数据的内存地址传给 I/O 调用函数，这样就避免了再去访问 Java 对象处理 I/O 读写。

**（1）MappedByteBuffer**

MappedByteBuffer 是 NIO 基于内存映射（mmap）这种零拷贝方式的提供的一种实现，意思是把一个文件从 position 位置开始的 size 大小的区域映射为内存映像文件。这样添加地址映射，而不进行拷贝。

**（2）DirectByteBuffer**

DirectByteBuffer 的对象引用位于 Java 内存模型的堆里面，JVM 可以对 DirectByteBuffer 的对象进行内存分配和回收管理，是 MappedByteBuffer 的具体实现类。因此同样具有零拷贝技术。

**（3）FileChannel**

FileChannel 定义了 transferFrom() 和 transferTo() 两个抽象方法，它通过在通道和通道之间建立连接实现数据传输的。

我们直接看Linux2.4的版本，socket缓冲区做了调整，DMA带收集功能。

（1）DMA从拷贝至内核缓冲区

（2）将数据的位置和长度的信息的描述符增加至内核空间(socket缓冲区)

（3）DMA将数据从内核拷贝至协议引擎

这个复制过程是零拷贝过程。

### **2、Netty**

Netty 中的零拷贝和上面提到的操作系统层面上的零拷贝不太一样, 我们所说的 Netty 零拷贝完全是基于（Java 层面）用户态的。

（1）Netty 通过 DefaultFileRegion 类对FileChannel 的 tranferTo() 方法进行包装，相当于是间接的通过java进行零拷贝。

（2）我们的数据传输一般都是通过TCP/IP协议实现的，在实际应用中，很有可能**一条完整的消息被分割为多个数据包进行网络传输，而单个的数据包对你而言是没有意义的，只有当这些数据包组成一条完整的消息时你才能做出正确的处理**，而Netty可以通过零拷贝的方式将这些数据包组合成一条完整的消息供你来使用。

此时零拷贝的作用范围仅在用户空间中。那Netty是如何实现的呢？为此我们就要找到Netty进行数据传输的接口，这个接口一定包含了可以实现零拷贝的功能，这个接口就是ChannelBuffer。

**既然有接口肯定就有实现类，一个最主要的实现类是CompositeChannelBuffer，这个类的主要作用是将多个ChannelBuffer组成一个虚拟的ChannelBuffer来进行操作**

**为什么说是虚拟的呢，因为CompositeChannelBuffer并没有将多个ChannelBuffer真正的组合起来，而只是保存了他们的引用，这样就避免了数据的拷贝，实现了Zero Copy。**

（3）ByteBuf 可以通过 wrap 操作把字节数组、ByteBuf、ByteBuffer 包装成一个 ByteBuf 对象, 进而避免了拷贝操作

（4）ByteBuf 支持 slice 操作, 因此可以将 ByteBuf 分解为多个共享同一个存储区域的 ByteBuf，避免了内存的拷贝

### **3、kafka**

Kafka 的索引文件使用的是 mmap + write 方式，数据文件使用的是 sendfile 方式。适用于系统日志消息这种高吞吐量的大块文件的数据持久化和传输。

如果有10个消费者，传统方式下，数据复制次数为4*10=40次，而使用“零拷贝技术”只需要1+10=11次，一次为从磁盘复制到页面缓存，10次表示10个消费者各自读取一次页面缓存。



# 软连接与硬连接

Linux 系统中有软链接和硬链接两种特殊的“文件”。

软链接可以看作是Windows中的快捷方式，可以让你快速链接到目标档案或目录。

硬链接则透过文件系统的inode来产生新档名，而不是产生新档案。

**创建方法都很简单：**

1. 软链接（符号链接） ln -s  source target 
2. 硬链接 （实体链接）ln    source  target

**区别：**
**1.硬链接原文件/链接文件公用一个inode号，说明他们是同一个文件，而软链接原文件/链接文件拥有不同的inode号，表明他们是两个不同的文件；**
**2.在文件属性上软链接明确写出了是链接文件，而硬链接没有写出来，因为在本质上硬链接文件和原文件是完全平等关系；**
**3.链接数目是不一样的，软链接的链接数目不会增加；**
**4.文件大小是不一样的，硬链接文件显示的大小是跟原文件是一样的。而这里软链接显示的大小与原文件就不同了，BBB大小是95B，而BBBsoft是3B。因为BBB共有3个字符**

**5.软链接没有任何文件系统的限制，任何用户可以创建指向目录的符号链接**

总之，**建立软链接就是建立了一个新文件**。当访问链接文件时，系统就会发现他是个链接文件，它读取链接文件找到真正要访问的文件。

**当然软链接也有硬链接没有的缺点：**因为链接文件包含有原文件的路径信息，所以当原文件从一个目录下移到其他目录中，再访问链接文件，系统就找不到了，而硬链接就没有这个缺陷，你想怎么移就怎么移；还有它要系统分配额外的空间用于建立新的索引节点和保存原文件的路径。

## **inode**

要解释清楚两者的区别和联系需要先说清楚 linux 文件系统中的 inode 这个东西。当划分磁盘分区并格式化的时候，整个分区会被划分为两个部分，即inode区和data block(实际数据放置在数据区域中）这个inode即是（目录、档案）文件在一个文件系统中的唯一标识，需要访问这个文件的时候必须先找到并读取这个 文件的 inode。 Inode 里面存储了文件的很多重要参数，其中唯一标识称作 Inumber, 其他信息还有创建时间（ctime）、修改时间(mtime) 、文件大小、属主、归属的用户组、读写权限、数据所在block号等信息。

![img](https://pic002.cnblogs.com/images/2012/364912/2012031910100478.jpg)

通常会根据分区的用途来安排inode的数量（这是另外一个话题了），比如文件数量很多而文件都很小，则需要调增inode较大，以便能索引全部文件。否则将会出现这个分区并没有写满而无法写入任何文件的情况。

## **目录文件与档案文件**

目录文件：记录该目录下的文件名

档案文件：记录实际文件数据

inode本身并不记录文件名，文件名记录在目录文件的block当中，所以***新增、删除、更改文件名***与目录的W权限有关。因此当我们要读某个档案时，就务必经过其**目录**的inode和block，然后才能够找到待读取***档案的inode***号，最终才会读到正确的***档案block***内的数据。**系统是通过索引节点(而不是文件名)来定位每一个文件。**

目录inode（满足权限？） => 目录block => 档案inode（满足权限？） => 档案block

## **硬链接**

![img](https://pic002.cnblogs.com/images/2012/359867/2012061121030539.jpg)

多个档名对应同一个inode，硬链接只是在某个目录下新增一笔档名链 接到某个inode号码的关联记录而已。如果将上图中任何一个档名删除，档案的inode与block都还存在，依然还可以通过另一个档名来读取正确的档 案数据。此外，不论用哪一个档名来编辑，最终的结果都会写入相同的inode和block中，因此均能进行数据的修改。

## 软连接

![img](https://pic002.cnblogs.com/images/2012/359867/2012061121135267.jpg)

软连接就是建立一个独立的文件，而这个文件会让数据的读取指向它link的那个档案的档名，由于只是作为**指向的动作**，所以当来源档案被删除之后，软连接的档案无法开启，因为找不到原始档名。连结档的内容只有档名，根据档名链接到正确的目录进一步取得目标档案的inode，最终就能够读取到正确的数据。如果目标档案的原始档名被删除了那么整个环节就进行不下去了。